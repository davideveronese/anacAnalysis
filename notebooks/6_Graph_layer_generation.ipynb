{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation basato su attributi testuali\n",
    "Obiettivo di questo notebook è quello di generare per ogni PA e Aggiudicatario una rappresentazione in uno spazio multidimensionale. Questa rappresentazione verrà usata per generare un grafo G(V,E) dove l'insieme dei nodi V rappresenta PA e Aggiudicatari, mentre l'insieme degli archi E definisce relazioni di similarità tra i nodi.\n",
    "\n",
    "Si definisce di seguito la soluzione proposta.\n",
    "\n",
    "\n",
    "## Soluzione 1: creazione di vettori basati sul tf-idf\n",
    "\n",
    "Ogni bando di gara è caratterizzato da un campo *oggetto*. Quindi per ogni elemento *i* ∈ { PA ⋃  Fornitori }, dove PA (Fornitori) è l’insieme delle Pubbliche amministrazioni (Fornitori) presenti nel dataset Anac,  è possibile creare un documento sintetico d<sub>i</sub> ottenuto concatenando il testo di tutti gli oggetti delle gare appartenenti ad *i*.\n",
    "\n",
    "A partire da D = {d<sub>i</sub> | i ∈ { PA ⋃  Fornitori}} è possibile creare la matrice W ∈ R<sup>(n,m)</sup>, ottenuta calcolando il TF-IDF su D.\n",
    "\n",
    "In questo modo è possibile creare un grafo G_text(V, E) dove V = { PA ⋃ Fornitori } ed esiste un arco pesato e = {(u, v) ∈ E | p(e) = cos(w<sub>u</sub>, w<sub>v</sub>)}  solo e soltanto se cos(w<sub>u</sub>, w<sub>v</sub>) >= *threshold*.\n",
    "\n",
    "cos(w<sub>u</sub>, w<sub>v</sub>) ∈ [0,1], dove 0 implica che i documenti d<sub>u</sub>, d<sub>v</sub> sono completamente diversi, mentre 1 implica che i documenti hanno la stessa distribuzione di termini.\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "- Dati i ∈ {PA} e j ∈ {Fornitori}, è possibile che se tutte le gare vinte da j riguardassero sempre i come struttura proponente (PA) e se tutte le gare di i siano vinte da j allora d<sub>i</sub> == d<sub>j</sub>, quindi cos(w<sub>i</sub>, w<sub>j</sub>) = 1.\n",
    "\n",
    "- E’ possibile aggiungere al grafo G<sub>text</sub> nuovi nodi come l’insieme delle provincie e regioni. In questo caso è possibile associare ad ogni provincia (regione) i un documento sintentico d_i ottenuto concatenando tutti gli oggetti delle PA che insistono su i.  \n",
    "\n",
    "- La matrice W non gestisce casi si sinonimia e polisemia (problema risolto con la soluziione successiva);\n",
    "\n",
    "## Soluzione 2: creazione di vettori basati su word embedding dei top term\n",
    "\n",
    "   Come nel precedente caso per ogni elemento i ∈ { PA ⋃  Fornitori }, dove PA (Fornitori) è l’insieme delle Pubbliche amministrazioni (Fornitori) presenti nel dataset Anac,  è possibile creare un documento sintetico d<sub>i</sub> ottenuto concatenando il testo di tutti gli oggetti delle gare appartenenti ad i. \n",
    "\n",
    "Sia *L* l’insieme delle top n ∈ N parole più frequenti (in termine di tf-idf)  e *D* = {d’<sub>i</sub> | i ∈ { PA ⋃  Fornitori } }, dove d’<sub>i</sub> = {(l, w) | l ∈ L e w = TF-IDF(l, d<sub>i</sub>)}. \n",
    "\n",
    "In questo modo per ogni i è possibile costruire un vettore v<sub>i</sub>  dato dalla somma pesata degli embedding dei termini l contenuti in d’<sub>i</sub>. \n",
    "Per esempio, sia  L = [“fattura”, “acquisto”, “fornitura”] l’insieme dei top 3 termini in D, e sia D ={d<sub>1</sub>, d<sub>2</sub>, d<sub>3</sub>}, dove:\n",
    "\n",
    "- d<sub>1</sub> = [(“acquisto”: w(1,acquisto)), (“fattura”: w(1,fattura))]  \n",
    "- d<sub>2</sub> = [(“fattura”: w(2,fattura)), (“fornitura”: w(2,fornitura))]  \n",
    "- d<sub>3</sub> = [(“acquisto”: w(3,acquisto)), (“fattura”: w(3,fattura)), (“fornitura”: w(3,fornitura))]\n",
    "\n",
    "Quindi è possibile per ogni d<sub>i</sub> calcolare una rappresentazione vettoriale\n",
    "\n",
    " \n",
    "$\\sum_{(tj,wj)∈di}$ w<sub>j</sub> * embedding(t<sub>j</sub>)\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "dove embedding(t<sub>j</sub>) è l’embedding del termine t<sub>j</sub> appreso su Wikipedia (vedi [qui](https://fasttext.cc/docs/en/support.html) per maggiori dettagli).\n",
    "\n",
    "In questo modo è possibile creare un grafo G<sub>text</sub>(V, E) dove V = { PA ⋃ Fornitori } ed esiste un arco pesato e = {(u, v) ∈ E | p(e) = cos(v<sub>du</sub>, v<sub>dv</sub>)}  solo e soltanto se cos(v<sub>du</sub>, v<sub>dv</sub>) >= threshold.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='.*/IPython/.*')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import csv\n",
    "import datetime\n",
    "from time import time\n",
    "import timeit\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.plotly as py_on\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/anac/anac_indicepa_2017.tsv\"\n",
    "df = pd.read_csv(file, sep=\"\\t\")\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dei documenti sintetici d<sub>i</sub>\n",
    "\n",
    "In questa sezione, per ogni attributo in *columns_to_aggregate*, è calcolato il suo documento sintetico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_aggregate = {(\"cfStrutturaProponente\",\"PA\"), (\"cfPrimoaggiudicatario\",\"AGG\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregateByOggetto(col_groupBy, col_toAgg, df):\n",
    "    \"\"\"\n",
    "    col_groupBy: è la colonna per la quale effettuare l'aggragazione\n",
    "    col_toAgg: è la colonna contenente le informazioni non strutturate, i.e. campo \"oggetto\"\n",
    "    df: input dataframe\n",
    "    \"\"\"\n",
    "    df_noEmpty = df.dropna(subset=[col_toAgg], axis=0)\n",
    "    new_series = df_noEmpty.groupby(col_groupBy)[col_toAgg].apply(lambda x: ' '.join(x)) \n",
    "    return pd.DataFrame(new_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns=['codiceFiscaleStruttura', 'oggetto', \"tipoStruttura\"])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, type_col in columns_to_aggregate:\n",
    "    t0 = time()\n",
    "    aggregate_df = aggregateByOggetto(col, \"oggetto\", df) \n",
    "    total = time() - t0\n",
    "\n",
    "    aggregate_df['tipoStruttura'] = [type_col] * aggregate_df.shape[0]\n",
    "    aggregate_df.rename(columns={col: 'codiceFiscaleStruttura'})\n",
    "\n",
    "    final_df = final_df.append(aggregate_df)\n",
    "    \n",
    "    print(\"Aggregation of attribute %s done in %0.3f sec\" % (col, total))\n",
    "    print(aggregate_df[\"oggetto\"].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "file_name = \"data/oggetti_aggregati.pickle\"\n",
    "final_df.to_pickle(file_name)\n",
    "total = time() - t0\n",
    "\n",
    "print(\"Dataset stored in %0.3f sec\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rilascia la memoria associata al DataFrame\n",
    "del [[df, final_df]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementazione Soluzione 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import ItalianStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix #need this if you want to save tfidf_matrix\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    \n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([italian_stemmer.stem(w) for w in analyzer(doc)])\n",
    "        \n",
    "def get_TFIDFmatrix_vect(data, do_stemming):\n",
    "    \"\"\"\n",
    "    data: input textual collection\n",
    "    do_stemming: boolean. If True execute stemming, otherwise analyze only tokenized words (words are composed at least 2 chars and do not contains numbers)\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    \n",
    "    if do_stemming:\n",
    "        \n",
    "        italian_stemmer = ItalianStemmer()\n",
    "        tf = StemmedCountVectorizer(token_pattern=u'([a-z]{2,})', max_features=1000, analyzer=\"word\", stop_words=stopwords.words('italian'))\n",
    "\n",
    "    else:\n",
    "        tf = TfidfVectorizer(token_pattern=u'([a-z]{2,})', stop_words=stopwords.words('italian'), max_features=1000) #CountVectorizer supports counts of N-grams of words or consecutive characters.\n",
    "    \n",
    "    matrix = tf.fit_transform(data)  \n",
    "    return matrix, tf\n",
    "\n",
    "def plotTopNWords(sorted_frequents_words, N, title):\n",
    "    \"\"\"frequents_words: list of frequents words. Type: tuple(str, numpy.int64)\n",
    "       N: number of words to plot \n",
    "    \"\"\"\n",
    "    #most_frequents_words = sorted(sorted_frequents_words, key = lambda x: x[1], reverse=True)\n",
    "    x, y = zip(*sorted_frequents_words[0:N]) # unpack a list of pairs into two tuples\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)  # Create matplotlib axes\n",
    "    ax1.set_title(title)\n",
    "    ax1.plot(x, y)\n",
    "\n",
    "    for tl in ax1.get_xticklabels():\n",
    "        tl.set_rotation(90)\n",
    "\n",
    "    file_name = 'imgs/top'+str(N)+'_words_plot.png'  \n",
    "    fig.tight_layout() \n",
    "    plt.savefig(file_name, pad = 0) #png\n",
    "\n",
    "    plt.clf\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/oggetti_aggregati_labels.pickle\"\n",
    "\n",
    "t0 = time()\n",
    "df = pd.read_pickle(file_name)\n",
    "total = time() - t0\n",
    "\n",
    "print(\"Dataset loaded in %0.3f sec\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "matrix, tf = get_TFIDFmatrix_vect(df.oggetto, False)\n",
    "total = time() - t0\n",
    "print(\"TFIDF matrix done in %0.3f sec\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = 40\n",
    "\n",
    "#Tf-Idf distribution\n",
    "scores = zip(tf.get_feature_names(), np.asarray(matrix.sum(axis=0)).ravel())\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "file_name = plotTopNWords(sorted_scores, top_terms,  \"Word Distribution based on TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "t0 = time()\n",
    "svd = TruncatedSVD(100)\n",
    "normalizer = Normalizer(copy=False)\n",
    "preprocessing = make_pipeline(svd, normalizer).fit_transform(matrix)\n",
    "total = time() - t0\n",
    "print(\"Preprocessing done in done in %0.3f sec\" % total)\n",
    "\n",
    "np.save(\"data/fasttext/matrix_tfidf.txt\", preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementazione Soluzione 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "jp_model = KeyedVectors.load_word2vec_format('data/fasttext/cc.it.300.vec.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,df.shape[0]):\n",
    "top_words = 300\n",
    "n_rows = df.shape[0]\n",
    "vocabulary = tf.vocabulary_\n",
    "terms, frequency = zip(*sorted_scores)\n",
    "\n",
    "embedded_matrix = np.empty((n_rows,300), float)\n",
    "t0 = time()\n",
    "\n",
    "for i in range(0, n_rows):\n",
    "    \n",
    "    embedding_d = [0] * 300\n",
    "    counter_d = 0\n",
    "    \n",
    "    for term in terms[:top_words]:\n",
    "        if term in jp_model.vocab:\n",
    "\n",
    "            index_t = vocabulary[term]\n",
    "            tfidf = matrix[i,index_t]\n",
    "            if(tfidf != 0):\n",
    "                embedding_d +=  jp_model.get_vector(term)\n",
    "                counter_d += 1\n",
    "    \n",
    "    if counter_d > 0:\n",
    "        embedding_d = embedding_d / counter_d\n",
    "    \n",
    "    embedded_matrix[i] = embedding_d #np.append(embedded_matrix, embedding_d, 0)\n",
    "total = time() - t0 \n",
    "print(\"Embedding done in %0.3f sec\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/fasttext/matrix_embedding.txt\", embedded_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costruzione del Grafo\n",
    "\n",
    "A partire dalla matrice dei documenti sintetici calcolata nelle precedenti sezioni (tf-idf matrix o embedding matrix) è possibile generate il  grafo G<sub>text</sub>(V, E) dove V = { PA ⋃ Fornitori } ed esiste un arco pesato e = {(u, v) ∈ E | p(e) = cos(v<sub>du</sub>, v<sub>dv</sub>)}  solo e soltanto se cos(v<sub>du</sub>, v<sub>dv</sub>) >= *t*.\n",
    "\n",
    "Impostiamo la threshold *t* = 0.6 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libera memoria\n",
    "#una volta lanciato questo comando verranno perse le variabili\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import csv\n",
    "import datetime\n",
    "from time import time\n",
    "import timeit\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.plotly as py_on\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(data):\n",
    "    \"\"\" calculate the (1-cosine) similarity among rows. \n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            yield i, j, 1 - spatial.distance.cosine(data[i], data[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/fasttext/matrix_embedding.txt.npy\"\n",
    "matrix = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11396 all zero rows\n"
     ]
    }
   ],
   "source": [
    "#check how many rows are all zero (it generates divisions by zero)\n",
    "zero_rows = len(np.where(~matrix.any(axis=1))[0])\n",
    "print(\"there are %d all zero rows\" % zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_iterator = cosine_similarity(matrix[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "#cosine_matrix = lil_matrix((matrix.shape[0], matrix.shape[0]))\n",
    "cosine_matrix = lil_matrix((1000, 1000))\n",
    "counter = 0\n",
    "for x in cosine_iterator:\n",
    "    if(x[2]>= 0.6):\n",
    "        cosine_matrix[x[0],x[1]] = x[2]\n",
    "        counter += 1\n",
    "        \n",
    "total = time() - t0\n",
    "print(\"Cosine Matrix done in %0.3f sec\" % total)\n",
    "print(\"There are %d pairs with a cosine similarity greater than 0.6\" % counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05933713912963867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/home/fabiana/miniconda3/envs/anac/lib/python3.6/site-packages/scipy/spatial/distance.py:649: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from scipy import spatial\n",
    "from time import time\n",
    "\n",
    "n = matrix.shape[0]\n",
    "lst = ((i, j) for i in range(0, n) for j in range(i+1,n))\n",
    "\n",
    "def cosine(p):\n",
    "    i = p[0]\n",
    "    j = p[1]\n",
    "    \n",
    "    if spatial.distance.cosine(matrix[i], matrix[j]) > 0.6:\n",
    "        return i,j, 1 - spatial.distance.cosine(matrix[i], matrix[j])\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "t0 = time()\n",
    "p = Pool(6)\n",
    "a = p.imap(cosine, lst)\n",
    "total = time() - t0\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = filter(None, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for el in a:\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
